import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.decomposition import PCA

from env import filtered, filter_reliable_predictions, make_data, handle_progress
# ---------------- HÃ m váº½ káº¿t quáº£ phÃ¢n loáº¡i ----------------
def plot_per_classifier(x_train, x_test, y_train, y_test, classifiers):
    if x_train.shape[1] > 2:
        X_all = np.vstack((x_train, x_test))
        X_all_2d = PCA(n_components=2).fit_transform(X_all)
        x_test_2d = X_all_2d[len(x_train):]
        print("ðŸ”· Dá»¯ liá»‡u > 2 chiá»u â†’ dÃ¹ng PCA Ä‘á»ƒ trá»±c quan.")
    else:
        x_test_2d = x_test
        print("ðŸ”· Dá»¯ liá»‡u Ä‘Ã£ lÃ  2 chiá»u â†’ khÃ´ng dÃ¹ng PCA.")

    num_models = len(classifiers)
    fig, axes = plt.subplots(num_models, 2, figsize=(14, 5 * num_models))
    if num_models == 1:
        axes = np.expand_dims(axes, axis=0)

    for idx, (name, model) in enumerate(classifiers.items()):
        model.fit(x_train, y_train)
        y_pred = model.predict(x_test)
        acc = accuracy_score(y_test, y_pred) * 100
        print(f"âœ… {name} - Accuracy: {acc:.2f}%")

        # Váº½ scatter
        ax_s = axes[idx, 0]
        ax_s.scatter(x_test_2d[:, 0], x_test_2d[:, 1], c=y_pred, cmap='tab10', s=20)
        ax_s.set_title(f'{name} - Dá»± Ä‘oÃ¡n (Acc: {acc:.2f}%)')
        ax_s.set_xlabel('Dim 1')
        ax_s.set_ylabel('Dim 2')
        ax_s.grid(True)

        # Váº½ cá»™ng dá»“n
        point = np.where(y_pred == y_test, 1, -1)
        cum_point = np.cumsum(point)

        ax_c = axes[idx, 1]
        ax_c.plot(cum_point, label='Cá»™ng dá»“n Ä‘iá»ƒm Ä‘Ãºng/sai')
        ax_c.axhline(0, color='gray', linestyle='--')
        ax_c.set_title(f'{name} - Cá»™ng dá»“n Ä‘Ãºng/sai')
        ax_c.set_xlabel('Máº«u')
        ax_c.set_ylabel('Tá»•ng Ä‘iá»ƒm')
        ax_c.grid(True)
        ax_c.legend()

    fig.tight_layout()
    fig.suptitle("So sÃ¡nh cÃ¡c thuáº­t toÃ¡n phÃ¢n loáº¡i", fontsize=18, y=1.02)
    # plt.show()


# ---------------- HÃ m dá»± Ä‘oÃ¡n máº«u má»›i ----------------
def predict_new_point(x_new, scaler, classifiers, x_train, x_test, y_train):
    if scaler:
        x_new_scaled = scaler.transform([x_new])
    else:
        x_new_scaled = [x_new]

    # Kiá»ƒm tra lá»c
    y_new_pred = is_pass_filtered(scaler, x_new, X, true_labels)
    if y_new_pred is None:
        return

    print(f"ðŸŽ¯ Máº«u má»›i vÆ°á»£t qua lá»c. Tiáº¿n hÃ nh dá»± Ä‘oÃ¡n báº±ng cÃ¡c mÃ´ hÃ¬nh...")

    # PCA náº¿u cáº§n
    if x_train.shape[1] > 2:
        X_all = np.vstack((x_train, x_test, x_new_scaled))
        X_all_2d = PCA(n_components=2).fit_transform(X_all)
        x_test_2d = X_all_2d[len(x_train):-1]
        x_new_2d = X_all_2d[-1]
    else:
        x_test_2d = x_test
        x_new_2d = x_new_scaled[0]

    # Váº½ Ä‘iá»ƒm má»›i trÃªn tá»«ng mÃ´ hÃ¬nh
    num_models = len(classifiers)
    fig, axes = plt.subplots(num_models, 1, figsize=(10, 5 * num_models))
    if num_models == 1:
        axes = [axes]

    for idx, (name, model) in enumerate(classifiers.items()):
        model.fit(x_train, y_train)
        y_pred_test = model.predict(x_test)
        y_pred_new = model.predict([x_new_scaled[0]])[0]

        print(f"ðŸ” {name}: dá»± Ä‘oÃ¡n máº«u má»›i lÃ  â†’ {y_pred_new}")

        ax = axes[idx]
        ax.scatter(x_test_2d[:, 0], x_test_2d[:, 1], c=y_pred_test, cmap='tab10', s=20, label='Táº­p test')
        ax.scatter(x_new_2d[0], x_new_2d[1], c='red', s=120, marker='X', label='Máº«u má»›i')
        ax.set_title(f'{name} - NhÃ£n dá»± Ä‘oÃ¡n máº«u má»›i: {y_pred_new}')
        ax.set_xlabel('Dim 1')
        ax.set_ylabel('Dim 2')
        ax.grid(True)
        ax.legend()

    fig.tight_layout()
    fig.suptitle("Dá»± Ä‘oÃ¡n vÃ  vá»‹ trÃ­ máº«u má»›i trÃªn cÃ¡c mÃ´ hÃ¬nh", fontsize=18, y=1.02)
    plt.show()


#thu x,y->split-> filter(x_train)->filter(x_test)->percent
def my_predict(msg):
    x_pred = handle_progress(msg, isEnd=False)
    x_pred = scaler.transform([x_pred])
    x_pred = np.round(x_pred, 1)
    if x_train is None:
        load_polot_data()

    y_pred, _, _ = filter_reliable_predictions(x_train, x_pred, [0], knn)
    if len(y_pred)==0:
        return 1, 0
    c1 = 0
    c2 = 0
    for idx, (name, model) in enumerate(classifiers.items()):
        y_pred = int(model.predict(x_pred)[0])
        print(name, y_pred)
        if y_pred == 1:
            c1+=1
        else:
            c2+=1
    if c1>c2:
        return 1, c1-c2
    return 2, c2-c1

scaler, data, label= make_data()
classifiers = {
    "KNN": KNeighborsClassifier(n_neighbors=5),
    "DecisionTree": DecisionTreeClassifier(),
    "RandomForest": RandomForestClassifier()
}

plot_data = {}
x_train = None
knn = None
def load_polot_data():
    global x_train, knn
    x_train, x_test, y_train, y_test = train_test_split(
    data, label, 
    train_size=0.19,
    test_size=0.019,
    shuffle=True,
    stratify=label)
    x_train, y_train, knn  = filtered(x_train, y_train, None)
    x_test, y_test, index = filter_reliable_predictions(x_train, x_test, y_test, knn)
    # x_test, y_test, knn = filtered(x_test, y_test, knn)
    for idx, (name, model) in enumerate(classifiers.items()):
        model.fit(x_train, y_train)
        y_pred = model.predict(x_test)
        acc = accuracy_score(y_test, y_pred) * 100
        print(f"âœ… {name} - Accuracy: {acc:.2f}%")

        # Danh sÃ¡ch Ä‘iá»ƒm Ä‘á»ƒ scatter
        scatter_points = []
        for i in range(len(x_test)):
            scatter_points.append({
                "x": float(x_test[i][0]),
                "y": float(x_test[i][1]),
                "pred": int(y_pred[i]),
                "true": int(y_test[i])
            })

        # Cá»™ng dá»“n Ä‘iá»ƒm Ä‘Ãºng/sai
        point = np.where(y_pred == y_test, 1, -1)
        cum_point = np.cumsum(point).tolist()  # chuyá»ƒn sang list JSON-compatible

        # Ghi vÃ o dict káº¿t quáº£
        plot_data[name] = {
            "accuracy": round(acc, 2),
            "scatter": scatter_points,
            "cumsum": cum_point
        }


# load_polot_data()


# Váº½ káº¿t quáº£ phÃ¢n loáº¡i
# plot_per_classifier(x_train, x_test, y_train, y_test, classifiers)

# Dá»± Ä‘oÃ¡n vÃ  váº½ Ä‘iá»ƒm má»›i
# x_new = [100, 110233242 - 192246358]  # báº¡n cÃ³ thá»ƒ thay báº±ng báº¥t ká»³ giÃ¡ trá»‹ nÃ o
# predict_new_point(x_new, scaler, classifiers, x_train, x_test, y_train)
